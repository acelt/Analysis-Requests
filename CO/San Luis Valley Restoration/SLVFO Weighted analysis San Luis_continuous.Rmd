---
title: "San Luis Valley Watershed Assessment"
author: "Alex Traynor"
date: "5/13/2021"
output:
  html_document:
    df_print: paged
    output: cosmo
  pdf_document: default
---
## Description
* This is the script for a weighted analysis for two reportinung unit: the San Luis and Alamosa-Trinchera HUC 8 watershed located within the San Luis Valley and Taos field offices. This analysis was requested from COSO, namely Gordon Toevs and Ben Billings. More information and notes from teh first few meetings describing this analysis can be found on the AIM Sharepoint [here.](https://doimspp.sharepoint.com/:w:/r/sites/ext-blm-oc-naim/Shared%20Documents/AIM%20Projects/Terrestrial%20Projects/Colorado/San%20Luis%20Valley%20FO/Analysis%20and%20Reporting/SLVFO%20Restoration%20Landscape%20Reporting%20Planning.docx?d=w718e4961579c4288b3d035b3099e2e8c&csf=1&web=1&e=xxeWJK)

* This analysis focuses on two land health standards (Colorado standards 1 and 3). Benchmarks were created by the SLVFO ID team using ecological site descriptions, AIM data, and professional knowledge. Benchmarks are described [here.](https://doimspp.sharepoint.com/:b:/r/sites/ext-blm-oc-naim/Shared%20Documents/AIM%20Projects/Restoration%20Landscapes/CO/Analysis%20and%20Reporting/Benchmarks%20and%20AIM%20Integration%20with%20Land%20Health%20Standards_SLVFO_2020.pdf?csf=1&web=1&e=tEeLV3)


## Setup
```{r setup, message=FALSE}

.libPaths("C:/R-4.2.1/library")

#devtools::install_git(url = "https://github.com/nstauffer/aim.analysis/", ref = "master")

### Package Setup
# Set local library path and load required packages
library(aim.analysis)
library(sp)
library(dplyr)
library(stringr)
library(sf)
library(rgdal)
library(tidyverse)
library(arcgisbinding)
arc.check_product()
```

### San Luis Watershed
### Set variables and input parameters

```{r input variables, message=FALSE}
#Alber's Equal Area is useful for calculating areas
projection <- sp::CRS("+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")
#Analysis date
date <- Sys.Date()
# Confidence level in percent
confidence <- 80
output_path <- "C:\\Users\\alaurencetraynor\\Documents\\CO\\SLVFO Restoration\\outputs"
# Filename for benchmark tool from analysis requester
benchmarks_filename <- "Monitoring Objectives SLVFO.xlsx"
# Core filename for all outputs/ name of reporting unit
output_filename <- "slvfo_sanluis"
# The variable in AIM sample designs that contains the unique identifier for each point
aim_idvar <- "PlotID"
# The variable in AIM sample designs that contains the fate for each point
aim_fatevar = "EvalStatus"
# The variable in TerrADat that contains the unique identifier for each point matching AIM designs'
tdat_idvar <- "PrimaryKey"
# The fates in sample designs which correspond to sampled/observed
observed_fates = c("TS", "Target Sampled", "Sampled", "Eval")
# The fates in sample designs which correspond to unneeded
# (e.g. unused oversample or points designated for future sampling)
invalid_fates = c("NN", NA, "Non Needed", "", "Not Sampled")
#sdd points
design_points_name <- "CO_rejections_san_luis_strata"
#terradat clipped points
sampled_points_name <- "Benchmarked_points_sanluis"
# The variable that contains the areas in HECTARES in wgtcats
wgtcat_area_var = "area_acres_aea"
# Reporting unit polygon/layer
wgtcats_name <- "SanLuis_poststrata"
# Geodatabase containing all spatial files
analysis_gdb <- "Weighted Analysis Inputs.gdb"
# The unique ID for poststratification polygons
wgtcats_var <- "wgtcat"
### The filepath to the folder where we have all the spatial data
path_spatial <- "U:\\My Documents\\Analysis\\Restoration\\SLVFO Restoration\\"
### The filepath to the folder where we have the Excel stuff
path_tabular <- "U:\\My Documents\\Analysis\\Restoration\\SLVFO Restoration\\"
#my favourite function
`%notin%` <- Negate(`%in%`)
#benchmark group var in design points
benchmark_var <- "BenchmarkGroup"
```

## Import Spatial and Tabular Data
### Read in weight categories
```{r read strata, message=FALSE, warning=FALSE}
#### Strata polygons
wgtcat_df <-sf::st_read(dsn = paste(path_spatial, analysis_gdb, sep = "/"),
                        layer = wgtcats_name,
                        stringsAsFactors = FALSE)
## Standardize projection
wgtcat_df <- sf::st_transform(wgtcat_df,
                              crs = projection)

## Make spatial 
wgtcat_spdf <- methods::as(wgtcat_df, "Spatial")
wgtcat_spdf@data[["wgtcat"]] <- wgtcat_spdf@data[[wgtcats_var]]
sf::st_geometry(wgtcat_df) <- NULL

wgtcat_df[["wgtcat"]] <- wgtcat_df[[wgtcats_var]]
```
### Read design points
```{r read design points, message=FALSE, warning=FALSE}
#### Read in design points
# Make sure points and polygons do not include Z/M dimensions...
design_points <- sf::st_read(dsn = paste(path_spatial, analysis_gdb, sep = "/"),
                             layer = design_points_name,
                             stringsAsFactors = FALSE)
## Standardize projection
design_points <- sf::st_transform(design_points,
                                  crs = projection)
## Make spatial 
design_points <- methods::as(design_points, "Spatial")

# Standardise columns
design_points@data[["unique_id"]] <- design_points@data[[aim_idvar]]
design_points@data[["fate"]] <- design_points@data[[aim_fatevar]]
design_points@data[["wgtcat"]] <- design_points@data[[wgtcats_var]]
design_points@data[["Benchmark.Group"]] <- NA # these are just rejections so dont have benchmark group

```
### Read benchmarks

```{r read benchmarks, message=FALSE}
#### Benchmarks
# Ive alreayd applied these in ArcPro but they may come in handy for figures and whatnot
# make sure all columns (including required proportions) are filled out - otherwise apply_benchamrks() will give errors
benchmarks <- read_benchmarks(filename = benchmarks_filename,
                              filepath = path_tabular)

benchmarks$Reporting.Unit <- output_filename
```

### Read Sampled points
```{r sampled points, message=FALSE, warning=FALSE}
#### Read in Terradat points
# Make sure points and polygons do not include Z/M dimensions...
sampled_points <- sf::st_read(dsn = paste(path_spatial, analysis_gdb, sep = "/"),
                              layer = sampled_points_name,
                              stringsAsFactors = FALSE)
## Standardize projection
sampled_points <- sf::st_transform(sampled_points,
                                   crs = projection)
## Make spatial 
sampled_points <- methods::as(sampled_points, "Spatial")

sampled_points@data[["fate"]] <- "TS"

# Standardise columns
sampled_points@data[["unique_id"]] <- sampled_points@data[[tdat_idvar]]
sampled_points@data[["wgtcat"]] <- sampled_points@data[[wgtcats_var]]
sampled_points@data[["wgtcat"]] <- as.character(sampled_points@data[["wgtcat"]])
sampled_points@data$Reporting.Unit <- output_filename

```


### Filter sampled points
```{r filter sampled points, message=FALSE}
# double check and remove targeted points
summary(as.factor(sampled_points$Design))
# theres a bunch of NAs I need to cross reference with the SDD
# for now well label them as unknown
sampled_points$Design[is.na(sampled_points$Design)] <- "Unknown"

sampled_points <- sampled_points[sampled_points$Design != "Random Targeted" & sampled_points$Design != "Targeted", ]

# theres a couple points here that were sampled miles away from the desing point - well remove those also
targeted <- c("18060411490727722018-09-01","CO_SLVFO_LUP_2020_HILL_011_V22022-09-01")

sampled_points <- sampled_points[!sampled_points$PrimaryKey %in% targeted,]

# double check sampling order and design holes
# not applicable here since we are looking at just a subset of the design
# looking at this by project and plot id which is not the best but gets at holes within a year
# project_list <- list()
# 
# for(project in sampled_points$ProjectName){
#   # find the number in plotid
#   data <- sampled_points[sampled_points$ProjectName == project, c("PlotID", "DateVisited")]
#   data$plot_order <- regmatches(data$PlotID, regexpr("[[:digit:]]+",data$PlotID))
#   project_list[project] <- data
#   # check to see if higher numbers were sampled sequentially
# }

# remove points that didnt fall in a benchmark group - well also need to double check these polys are removed from the inference area
# we can check this by looking for nulls in benchmark fields
benchmark_fields <- unique(benchmarks$Objective.name)

#KEEPING THESE IN FOR CONT ANALYSIS
# for(field in benchmark_fields){
#   sampled_points <- sampled_points[!is.na(sampled_points@data[[field]]),]
# }

### Combine sampled points with rejections
# add benchmark fields to rejections
for(col in benchmark_fields){
  design_points[[col]] <- NA
}

all_points <- rbind(design_points[, c("unique_id", "fate", "wgtcat", benchmark_fields)], 
                    sampled_points[, c("unique_id", "fate", "wgtcat", benchmark_fields)])

# there one rejection that doesnt have a specific criteria
# it should be Inaccessbile
all_points$fate[all_points$fate == "Rejected"] <- "Inaccessible"
# theres an issue with the weight gen function below so updating to shortern versions of rejection 
all_points$fate[all_points$fate == "Inaccessible"] <- "IA"
all_points$fate[all_points$fate == "Non-Target"] <- "NT"
```

## Weighting
```{r point weighting, message=FALSE, warning=FALSE}
###### Analyze ######
weight_info <- weight.gen(pts = all_points,# remove targeted plots
                          pts.fatefield = "fate",
                          pts.groupfield = "wgtcat",
                          frame.spdf = wgtcat_spdf,
                          frame.groupfield = "wgtcat",
                          target.values = c("Target Sampled", "TARGET SAMPLED", "TS"),
                          unknown.values = c("Unknown", "UNKNOWN", "UNK"),
                          nontarget.values = c("NT"),
                          inaccessible.values = c("IA"),
                          unneeded.values = invalid_fates)

point_weights <-  weight_info[["point.weights"]]

all_points <-  aim.analysis::add_coords(all_points,
                                        xynames = c("x", "y"))
```

### Organise Indicators
```{r}
# make indicators long instead of wide
benchmarked_points <- all_points@data %>% 
  pivot_longer(cols = benchmark_fields, names_to = 'Indicator', values_to = "Condition.Category")

# remove the unsamplEd plots
benchmarked_points <- benchmarked_points[!is.na(benchmarked_points$Condition.Category),]

#Add reporting unit
all_points[["Reporting.Unit"]] <- output_filename
weight_info[["point.weights"]][["Reporting.Unit"]] <- output_filename
# need to add reporting unit name to analyse
point_weights$Reporting.Unit <- output_filename
```

### Category Definitions
```{r prep for analyze, message=FALSE}
# a definitions list for analyze_cat_multi()
# this is just a list of all the possible condition categorys
cat_defs <-  benchmarks[,c("Benchmark.Group", "Objective.name", "Condition.Category")]

colnames(cat_defs) <- c("Benchmark.Group","Indicator", "Condition.Category")
cat_defs <- unique(cat_defs)
```

## Analyse
```{r analyze, message=FALSE}
analysis <- analyze_cat_multi(data = benchmarked_points,
                        weights = point_weights,
                        id_var = "unique_id",
                        cat_var = "Condition.Category",
                        wgt_var = "WGT",
                        split_vars = "Indicator",
                        definitions = cat_defs,
                        conf = confidence,
                        verbose = FALSE)
```

### Confidence Intervals
```{r confidence intervals, message=FALSE}
# Calculate CIs for acres
total_acres = (analysis$total_observation_weight/analysis$weighted_observation_proportion)[1]

analysis <- analysis %>% 
  mutate(lower_bound_acres = weighted_observation_proportion_lower_bound_80pct*total_acres,
         upper_bound_acres = weighted_observation_proportion_upper_bound_80pct*total_acres)

analysis$Reporting.Unit <- output_filename

names(analysis) <- c("Rating",
                     "Number of plots",
                     "Unweighted proportion of sampled area",
                     "Estimated hectares",
                     "Weighted proportion of sampled area",
                     paste0(c("Lower confidence bound of percent area (", "Upper confidence bound of percent area ("), confidence, "%, Goodman multinomial) "),
                     "Indicator",
                     paste0(c("Lower confidence bound of acres (", "Upper confidence bound of acres ("), confidence, "%, Goodman multinomial)"),
                     "Reporting Unit")

```

### Formatting Outputs
```{r formatting outputs, message=FALSE}
wgtcat_summary <- weight_info[["frame.summary"]]
names(wgtcat_summary) <- c("wgtcat",
                           "total_count",
                           "count_Target Sampled",
                           "count_NT",
                           "count_IA",
                           "count_NA",
                           "count_UNK",
                           "area_ha",
                           "observed_proportion",
                           "observed_area",
                           "weight")

fates <- c("NT","IA","NA","UNK")

for (fate in fates) {
  wgtcat_summary[[paste0("count_", fate)]][is.na(wgtcat_summary[[paste0("count_", fate)]])] <- 0
}

wgtcat_summary[["in_inference"]] <- (wgtcat_summary[["weight"]])>0

wgtcat_summary$area_units <- "acres"

wgtcat_summary$unobserved_area <- wgtcat_summary$area_ha - wgtcat_summary$observed_area

# Add in where they're from
all_points <- sp::merge(x = all_points,
                        y = point_weights[,c("unique_id","WGT")],
                        all.x = TRUE,
                        by = "unique_id")

# adding a field with sum of benchmarks meeting for all benchmarks and each individual standard
standard1_fields = colnames(all_points@data)[str_detect(colnames(all_points@data), "Standard1")]
standard3_fields = colnames(all_points@data)[str_detect(colnames(all_points@data), "Standard3")]

num_benchmarks <- length(benchmark_fields)
num_benchmarks_std1 <- length(standard1_fields)
num_benchmarks_std3 <- length(standard3_fields)

all_points@data <- all_points@data %>%
  mutate(sum_meeting = num_benchmarks - rowSums(across(.cols = all_of(benchmark_fields), .fns = str_detect, "Not Meeting")),
         sum_meeting_std1 = num_benchmarks_std1 - rowSums(across(.cols = all_of(standard1_fields), .fns = str_detect, "Not Meeting")),
         sum_meeting_std3 = num_benchmarks_std3 - rowSums(across(.cols = all_of(standard3_fields), .fns = str_detect, "Not Meeting")),
         percent_meeting = sum_meeting/num_benchmarks*100,
         percent_meeting_std1 = sum_meeting_std1/num_benchmarks_std1*100,
         percent_meeting_std3 = sum_meeting_std3/num_benchmarks_std3*100,
         benchmark_index = NA)


# reformat benchmark_points to wide format
wgtcat_spdf <- merge(x = wgtcat_spdf,
                     y = wgtcat_summary,
                     by = "wgtcat")

# Rounding numbers for clarity
wgtcat_spdf@data[,is.numeric(wgtcat_spdf@data)] <- round(is.numeric(wgtcat_spdf@data), 2)
```
## Results
### Benchmarks
```{r, message=FALSE}
benchmarks_table <- benchmarks %>% DT::datatable(extensions = 'Buttons', filter = "top" , 
                            options = list(scrollX = TRUE ,
                            dom = 'Bfrtip',
                            buttons =
                            list(list(
                            extend = 'collection',
                            buttons = c('csv', 'excel'),
                            text = 'Download Table'))) , 
                 caption = paste("Weighted Analysis Benchmarks for " , 
                                 toString(output_filename)) , 
                 rownames = FALSE)
benchmarks_table
```

### Point Weights
```{r, message=FALSE}
weights_table <- point_weights %>% DT::datatable(extensions = 'Buttons', filter = "top" , 
                            options = list(scrollX = TRUE ,
                            dom = 'Bfrtip',
                            buttons =
                            list(list(
                            extend = 'collection',
                            buttons = c('csv', 'excel'),
                            text = 'Download Table'))) , 
                 caption = paste("Point Weights for " , 
                                 toString(output_filename)) , 
                 rownames = FALSE)
weights_table
```

### Continuous analysis
```{r}
library(spsurvey)
# make sure sandy and salty groups are in here

# reduce down to the 16 indicators we care about
indicatorswecareabout <- unique(benchmarks$Indicator)

# organising dataframe for cont analysis
mysiteIDs <- all_points@data$unique_id[all_points@data$fate == 'TS']
mysites <-  data.frame(siteID = mysiteIDs,
                       Active = TRUE)

mywgtcats <- all_points@data$wgtcat[all_points@data$fate == 'TS'] 

# need to add a chartacter in front 
mywgtcats <- paste0("wgtcat", mywgtcats)

mysubpop <- data.frame(siteID = mysiteIDs)

# lets also also benchmark group as a subpop
mysubpop <- merge(x = mysubpop,
                  y = sampled_points@data[,c("PrimaryKey", "BenchmarkGroup")],
                  by.x = "siteID",
                  by.y = "PrimaryKey",
                  all.x = TRUE,
                  all.y = FALSE)

mywgts = all_points@data$WGT[all_points@data$fate == 'TS']
mydesign <- data.frame(siteID = mysiteIDs,
                       wgt = mywgts,
                       xcoord = all_points@data$x[all_points@data$fate == 'TS'],
                       ycoord = all_points@data$y[all_points@data$fate == 'TS'],
                       stratum = mywgtcats)

mydata.cont <- sampled_points@data[sampled_points@data$PrimaryKey %in% mysiteIDs, c("PrimaryKey", indicatorswecareabout)] %>% 
  dplyr::rename("siteID" = "PrimaryKey")

# for san Luis
# these are teh sizes of wgtcats within each benchmark group based on spatial intersection
popsize <- arc.select(arc.open(path = "C:\\Users\\alaurencetraynor\\Documents\\CO\\SLVFO Restoration\\inputs\\SLVFO Restoration.gdb\\SanLuis_poststrata_BG_intersection"))

# build the list of vectors spsurvey wants
BenchmarkGroup <- list()

popsize <- popsize %>% 
  mutate(wgtcats_name = paste0("wgtcat", wgtcat))

for(stratum in unique(popsize$stratum_nm)){
  acres <- popsize$area_acres_aea[popsize$stratum_nm == stratum]
  names <- popsize$wgtcats_name[popsize$stratum_nm == stratum]
  named_acres <- setNames(acres, names)
  BenchmarkGroup[[stratum]] <- named_acres
}

mypop.size <- list(BenchmarkGroup = BenchmarkGroup)

cont_analysis <- cont.analysis(sites=mysites, subpop=mysubpop, design=mydesign,
  data.cont=mydata.cont, popsize=mypop.size, conf = 80)

```

### Calculating Indices and Export
```{r}
# Plotting continuous results
cont_estimates <- cont_analysis$Pct[cont_analysis$Pct$Statistic == "Mean",]

# join benchmarks
cont_estimates_benchmarks <- merge(x=cont_estimates,
                                   y=benchmarks,
                                   by.x = c("Subpopulation", "Indicator"),
                                   by.y = c("Benchmark.Group", "Indicator"),
                                   all.x = TRUE) %>% 
  mutate(diff_lower = Estimate/Lower.Limit) 


# the index is going to work differently based on whether the indicator is better when lower, better when high or has an ideal range
# positive indicators = better when higher
positive <- c("NumSpp_NonNoxPlant","SoilStability_All","AH_NonNoxPerenForbCover","AH_NonNoxPerenGrassCover", "TotalFoliarCover")
negative <- c("BareSoilCover", "GapCover_25_50","GapCover_51_100","GapCover_101_200","GapCover_200_plus","AH_NoxAnnGrassCover",   "AH_NoxCover","NumSpp_NoxPlant")
neutral <- c("AH_NonNoxShrubCover", "AH_NonNoxTreeCover","FH_TotalLitterCover")

# the function for negative benchmarks
# benchmark (or lower) = 1
# 100 = 0 (as far away from the benchmark you could get)
# formula = DepartureIndex ~ (1/(Benchmark-100))* Estimate - (100 /(Benchmark-100)))

cont_estimates_departure <- cont_estimates_benchmarks %>% 
  filter(Condition.Category == "Meeting") %>% 
  mutate(Lower.Limit = ifelse(Lower.Limit == -1, 0, Lower.Limit)) %>% 
  mutate(DepartureIndex = ifelse(Indicator %in% positive,
                                 ifelse(diff_lower < 1,
                                        diff_lower,
                                        1),
                                 ifelse(Indicator %in% negative,
                                        ((1/(Lower.Limit-100))*Estimate) - (100/(Lower.Limit-100)),
                                        ifelse(Indicator %in% neutral,
                                               ifelse(Estimate >= Lower.Limit & Estimate <= Upper.Limit,
                                                      1,
                                                      ifelse(Estimate < Lower.Limit,
                                                             diff_lower,
                                                             ifelse(Estimate > Upper.Limit,
                                                                    ((1/(Upper.Limit-100))*Estimate) - (100/(Upper.Limit-100)),
                                                                    NA)
                                                             )
                                                      )
                                               ,NA)
                                        )
                                 )
         )                                 


# lets export the cont data too
write.csv(cont_estimates_departure,
          file = paste0(output_path, "/",
                        output_filename,
                        "_continuous_estimates_",
                        date, ".csv"),
          row.names = FALSE)

# make a wide table for pro
Index_by_BG <- cont_estimates_departure %>% 
  select(-c(Type, Statistic, StdError,Management.Question:diff_lower)) %>% 
  separate(Objective.name, c("one", "Standard", "three")) %>% select(-c(one, three)) %>% 
  group_by(Subpopulation, Standard) %>% 
  summarise(Mean_Index = mean(DepartureIndex))

Index_by_BG <- cont_estimates_departure %>% 
  select(-c(Type, Statistic, StdError,Management.Question:diff_lower)) %>% 
  separate(Objective.name, c("one", "Standard", "three")) %>% select(-c(one, three)) %>% 
  group_by(Subpopulation, Standard) %>% 
  summarise(Mean_Index = mean(DepartureIndex))

write.csv(Index_by_BG,file = paste0(output_path, "/",
                        output_filename,
                        "_index_benchmarkgroup_",
                        date, ".csv"),
          row.names = FALSE)
ggplot(data = Index_by_BG, aes(x = Subpopulation, y = Mean_Index, fill = Standard))+
  geom_col(position = "dodge")

cont_estimates_wide <- cont_estimates_departure %>% 
  select(-c(Type, Statistic, StdError,Objective.name:diff_lower)) %>% 
  pivot_wider(names_from = Indicator,
              values_from = c(NResp, Estimate,LCB80Pct, UCB80Pct, DepartureIndex)) 




# lets export the cont data too
write.csv(cont_estimates_wide,
          file = paste0(output_path, "/",
                        output_filename,
                        "_continuous_estimates_wide",
                        date, ".csv"),
          row.names = FALSE)

sampled_data_long <-  sampled_points@data %>% 
  select(all_of(indicatorswecareabout), PrimaryKey, BenchmarkGroup, Reporting.Unit) %>% 
  pivot_longer(cols = indicatorswecareabout, names_to = "Indicator", values_to = "Value")

# we should do this same thing with the plot data
sampled_points_benchmarked <-  merge(x= sampled_data_long,
                                     y= benchmarks[benchmarks$Condition.Category == "Meeting",],
                                     by.x = c("BenchmarkGroup", "Indicator"),
                                     by.y = c("Benchmark.Group", "Indicator"),
                                     all.x = TRUE) %>% 
  mutate(diff_lower = Value/Lower.Limit)

sampled_data_departure <- sampled_points_benchmarked %>% 
  mutate(Lower.Limit = ifelse(Lower.Limit == -1, 0, Lower.Limit)) %>% 
  mutate(DepartureIndex = ifelse(Indicator %in% positive,
                                 ifelse(diff_lower < 1,
                                        diff_lower,
                                        1),
                                 ifelse(Indicator %in% negative,
                                        ((1/(Lower.Limit-100))*Value) - (100/(Lower.Limit-100)),
                                        ifelse(Indicator %in% neutral,
                                               ifelse(Value >= Lower.Limit & Value <= Upper.Limit,
                                                      1,
                                                      ifelse(Value < Lower.Limit,
                                                             diff_lower,
                                                             ifelse(Value > Upper.Limit,
                                                                    ((1/(Upper.Limit-100))*Value) - (100/(Upper.Limit-100)),
                                                                    NA)
                                                             )
                                                      )
                                               ,NA)
                                        )
                                 )
         )                                 
# reformattin to make wide for review
# make wide and calc average departure
sampled_data_departure_wide <- sampled_data_departure %>% 
  select(-c(evalstring1,evalstring2,evalstring_threshhold, diff_lower, Objective.name, Management.Question,Benchmark.Source,Lower.Limit, LL.Relation,UL.Relation, Upper.Limit, Unit, Condition.Category,Proportion.Relation, Required.Proportion, Reporting.Unit.x)) %>% 
  rename("Reporting.Unit" = "Reporting.Unit.y") %>% 
  pivot_longer(cols = "DepartureIndex", names_to = "Indicator_DepartureIndex", values_to = "Value_DepartureIndex") 

departure <- sampled_data_departure_wide[,c("BenchmarkGroup","Indicator","Indicator_DepartureIndex","PrimaryKey", "Value_DepartureIndex","Reporting.Unit")] %>% 
  mutate(Indicator1 = paste0(Indicator_DepartureIndex,"_",Indicator)) %>% 
  select(-c(Indicator, Indicator_DepartureIndex)) %>% 
  rename(Indicator = Indicator1,
         Value = Value_DepartureIndex)

#drop departure cols
sampled_dep_drop <- sampled_data_departure_wide %>% 
  select(-c("Indicator_DepartureIndex","Value_DepartureIndex"))

# Indicator weights
weights_LHS1 = data.frame(BareSoilCover = 0.25,
            FH_TotalLitterCover = 0.13,
            GapCover_200_plus = 0.20,
            NumSpp_NonNoxPlant = 0.19,
            SoilStability_All = 0.22)

weights_LHS3 = data.frame(AH_NonNoxPerenForbCover = 0.10,
                 AH_NonNoxPerenGrassCover = 0.18,
                 AH_NonNoxShrubCover = 0.08,
                 AH_NonNoxTreeCover = 0.09,
                 AH_NoxCover = 0.09,
                 NumSpp_NonNoxPlant = 0.19,
                 NumSpp_NoxPlant = 0.10,
                 TotalFoliarCover = 0.19)
# now rbind
sampled_data_departure_wide <- rbind(sampled_dep_drop,departure) %>% 
  pivot_wider(id_cols = PrimaryKey, names_from = "Indicator", values_from = "Value")

weighted_index <- function(weights, dataframe){
  names(weights) <- paste0("DepartureIndex_", names(weights))
  indicators <- names(weights)
  
  weights <- weights %>% 
    pivot_longer(cols = everything(), names_to = "Indicator", values_to = "weight")
  
  data <- dataframe %>% 
    select("PrimaryKey", all_of(indicators)) %>% 
    pivot_longer(cols = all_of(indicators), names_to = "Indicator", values_to = "Value") %>% 
    left_join(y = weights, by = "Indicator")%>% 
    group_by(PrimaryKey) %>% 
    mutate(Index = weighted.mean(Value, weight, na.rm = TRUE)) %>% 
    select(-c(Indicator, Value, weight)) %>% 
    ungroup() %>% 
    unique()
  return(data)
}

weighted_index_LHS1 <- weighted_index(weights = weights_LHS1, sampled_data_departure_wide)

weighted_index_LHS3 <- weighted_index(weights = weights_LHS3, sampled_data_departure_wide)

# join to rest of data
sampled_data_departure_wide <- merge(x = sampled_data_departure_wide,
                                     y = weighted_index_LHS1,
                                     by = "PrimaryKey") %>% 
  rename(LHS1_Weighted_Mean_Index = Index)

sampled_data_departure_wide <- merge(x = sampled_data_departure_wide,
                                     y = weighted_index_LHS3,
                                     by = "PrimaryKey") %>% 
  rename(LHS3_Weighted_Mean_Index = Index)

# lets export the cont data too
write.csv(sampled_data_departure_wide,
          file = paste0(output_path, "/",
                        output_filename,
                        "_sampled_data",
                        date, ".csv"),
          row.names = FALSE)

```


### Figures
#### Indicator Histograms/CDFs
```{r, fig.width= 10, fig.height=8, message=FALSE, warning=FALSE}
library(ggplot2)
library(plotly)
library(scales)
# theme_excel_new

# Fancy palette from Nelson

palette = c("#f5bb57ff",
                                            "#f8674cff",
                                            "#4a8b9fff",
                                            "#685b7fff",
                                            "#c95294ff",
                                            "#f5a9c6ff",
                                            "#75c6c5ff",
                                            "#fd6794ff")

# cap nox cover error bars at 0 
cont_estimates <- cont_estimates %>% 
  mutate(LCB80Pct = ifelse(LCB80Pct < 0, 0, LCB80Pct))
  
  
ggplot(data = cont_estimates[cont_estimates$Indicator %in% indicatorswecareabout,], aes(y = Estimate, x = Subpopulation, col = Subpopulation))+
  geom_point()+
  geom_errorbar(aes(ymin = LCB80Pct,
                    ymax = UCB80Pct),
                width = 0.5)+
  facet_wrap(.~Indicator, scales = "free")+
  theme_bw(base_size = 14)+
  scale_color_discrete(type = palette)+
  theme(axis.text.x = element_text(angle = 30, vjust = 0.9, hjust = 1))+
  labs(x = "") +
  expand_limits(y=0)

ggsave(device = "jpeg", dpi = 400, height = 15, width = 15, filename = paste0("cont_analysis_", output_filename, ".jpeg"), path = output_path)

# individual pltos for vbetter scale
for(i in indicatorswecareabout){
  ggplot(data = cont_estimates[cont_estimates$Indicator == i,], aes(y = Estimate, x = Subpopulation, col = Subpopulation))+
  geom_point()+
  geom_errorbar(aes(ymin = LCB80Pct,
                    ymax = UCB80Pct),
                width = 0.5)+
  theme_bw(base_size = 14)+
  scale_color_discrete(type = palette)+
  theme(axis.text.x = element_text(angle = 30, vjust = 0.9, hjust = 1))+
  labs(x = "") +
  ggtitle(label = i)+
  expand_limits(y=0)

ggsave(device = "jpeg", dpi = 400, height = 4, width = 6, filename = paste0("cont_analysis_",i,"_", output_filename, ".jpeg"), path = output_path)
}

## CDF
cdfindicators <- indicatorswecareabout[!indicatorswecareabout %in% c("NumSpp_NonNoxPlant","NumSpp_NoxPlant")]

for(group in unique(cont_analysis$CDF$Subpopulation)){
  for(indicator in unique(cdfindicators)){
    data <- cont_analysis$CDF[cont_analysis$CDF$Subpopulation == group & cont_analysis$CDF$Indicator == indicator,]
    # might need to adjust parameters for the different indicators e.g.s remove percent label on soil stab
    jpeg(paste0(group,"_",indicator,".jpg")) 
    spsurvey::cdf.plot(data, figlab = paste0(group," - ",indicator), xlbl = "Estimate")
     dev.off()
  }
}


```
## Index figure
```{r}
hist(sampled_data_departure_wide$LHS1_Weighted_Mean_Index, xlab = "Standard 1 Weighted Mean Index", main = "Standard 1 Index - San Luis Sub-Basin")
hist(sampled_data_departure_wide$LHS3_Weighted_Mean_Index, xlab = "Standard 3 Weighted Mean Index", main = "Standard 3 Index - San Luis Sub-Basin")
```

