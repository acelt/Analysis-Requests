---
title: "San Luis Valley Watershed Assessment"
author: "Alex Traynor"
date: "5/13/2021"
output:
  html_document:
    df_print: paged
    output: cosmo
  pdf_document: default
---
## Description
* This is the script for a weighted analysis for two reportinung unit: the San Luis and Alamosa-Trinchera HUC 8 watershed located within the San Luis Valley and Taos field offices. This analysis was requested from COSO, namely Gordon Toevs and Ben Billings. More information and notes from teh first few meetings describing this analysis can be found on the AIM Sharepoint [here.](https://doimspp.sharepoint.com/:w:/r/sites/ext-blm-oc-naim/Shared%20Documents/AIM%20Projects/Terrestrial%20Projects/Colorado/San%20Luis%20Valley%20FO/Analysis%20and%20Reporting/SLVFO%20Restoration%20Landscape%20Reporting%20Planning.docx?d=w718e4961579c4288b3d035b3099e2e8c&csf=1&web=1&e=xxeWJK)

* This analysis focuses on two land health standards (Colorado standards 1 and 3). Benchmarks were created by the SLVFO ID team using ecological site descriptions, AIM data, and professional knowledge. Benchmarks are described [here.](https://doimspp.sharepoint.com/:b:/r/sites/ext-blm-oc-naim/Shared%20Documents/AIM%20Projects/Restoration%20Landscapes/CO/Analysis%20and%20Reporting/Benchmarks%20and%20AIM%20Integration%20with%20Land%20Health%20Standards_SLVFO_2020.pdf?csf=1&web=1&e=tEeLV3)


## Setup
```{r setup, message=FALSE}

.libPaths("C:/R-4.2.1/library")

#devtools::install_git(url = "https://github.com/nstauffer/aim.analysis/", ref = "master")

### Package Setup
# Set local library path and load required packages
library(aim.analysis)
library(sp)
library(dplyr)
library(stringr)
library(sf)
library(rgdal)
library(tidyverse)
library(scales)
library(plotly)
```

### San Luis Watershed
### Set variables and input parameters

```{r input variables, message=FALSE}
#Alber's Equal Area is useful for calculating areas
projection <- sp::CRS("+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")
#Analysis date
date <- Sys.Date()
# Confidence level in percent
confidence <- 80
output_path <- "C:\\Users\\alaurencetraynor\\Documents\\CO\\SLVFO Restoration\\outputs"
# Filename for benchmark tool from analysis requester
benchmarks_filename <- "Monitoring Objectives SLVFO.xlsx"
# Core filename for all outputs/ name of reporting unit
output_filename <- "slvfo_alamosatrinchera"
# The variable in AIM sample designs that contains the unique identifier for each point
aim_idvar <- "PlotID"
# The variable in AIM sample designs that contains the fate for each point
aim_fatevar = "EvalStatus"
# The variable in TerrADat that contains the unique identifier for each point matching AIM designs'
tdat_idvar <- "PrimaryKey"
# The fates in sample designs which correspond to sampled/observed
observed_fates = c("TS", "Target Sampled", "Sampled", "Eval")
# The fates in sample designs which correspond to unneeded
# (e.g. unused oversample or points designated for future sampling)
invalid_fates = c("NN", NA, "Non Needed", "", "Not Sampled")
#sdd points
design_points_name <- "CO_rejections_alamosa_trinchera_strata"
#terradat clipped points
sampled_points_name <- "Benchmarked_points_alamosa"
# The variable that contains the areas in HECTARES in wgtcats
wgtcat_area_var = "area_acres_aea"
# Reporting unit polygon/layer
wgtcats_name <- "Alamosa_trinchera_poststrata"
# Geodatabase containing all spatial files
analysis_gdb <- "Weighted Analysis Inputs.gdb"
# The unique ID for poststratification polygons
wgtcats_var <- "wgtcat"
### The filepath to the folder where we have all the spatial data
path_spatial <- "U:\\My Documents\\Analysis\\Restoration\\SLVFO Restoration\\"
### The filepath to the folder where we have the Excel stuff
path_tabular <- "U:\\My Documents\\Analysis\\Restoration\\SLVFO Restoration\\"
#my favourite function
`%notin%` <- Negate(`%in%`)
#benchmark group var in design points
benchmark_var <- "BenchmarkGroup"
```

## Import Spatial and Tabular Data
### Read in weight categories
```{r read strata, message=FALSE, warning=FALSE}
#### Strata polygons
wgtcat_df <-sf::st_read(dsn = paste(path_spatial, analysis_gdb, sep = "/"),
                        layer = wgtcats_name,
                        stringsAsFactors = FALSE)
## Standardize projection
wgtcat_df <- sf::st_transform(wgtcat_df,
                              crs = projection)

## Make spatial 
wgtcat_spdf <- methods::as(wgtcat_df, "Spatial")
wgtcat_spdf@data[["wgtcat"]] <- wgtcat_spdf@data[[wgtcats_var]]
sf::st_geometry(wgtcat_df) <- NULL

wgtcat_df[["wgtcat"]] <- wgtcat_df[[wgtcats_var]]
```
### Read design points
```{r read design points, message=FALSE, warning=FALSE}
#### Read in design points
# Make sure points and polygons do not include Z/M dimensions...
design_points <- sf::st_read(dsn = paste(path_spatial, analysis_gdb, sep = "/"),
                             layer = design_points_name,
                             stringsAsFactors = FALSE)
## Standardize projection
design_points <- sf::st_transform(design_points,
                                  crs = projection)
## Make spatial 
design_points <- methods::as(design_points, "Spatial")

# Standardise columns
design_points@data[["unique_id"]] <- design_points@data[[aim_idvar]]
design_points@data[["fate"]] <- design_points@data[[aim_fatevar]]
design_points@data[["wgtcat"]] <- design_points@data[[wgtcats_var]]
design_points@data[["Benchmark.Group"]] <- NA # these are just rejections so dont have benchmark group

```
### Read benchmarks

```{r read benchmarks, message=FALSE}
#### Benchmarks
# Ive alreayd applied these in ArcPro but they may come in handy for figures and whatnot
# make sure all columns (including required proportions) are filled out - otherwise apply_benchamrks() will give errors
benchmarks <- read_benchmarks(filename = benchmarks_filename,
                              filepath = path_tabular)

benchmarks$Reporting.Unit <- output_filename
```

### Read Sampled points
```{r sampled points, message=FALSE, warning=FALSE}
#### Read in Terradat points
# Make sure points and polygons do not include Z/M dimensions...
sampled_points <- sf::st_read(dsn = paste(path_spatial, analysis_gdb, sep = "/"),
                              layer = sampled_points_name,
                              stringsAsFactors = FALSE)
## Standardize projection
sampled_points <- sf::st_transform(sampled_points,
                                   crs = projection)
## Make spatial 
sampled_points <- methods::as(sampled_points, "Spatial")

sampled_points@data[["fate"]] <- "TS"

# Standardise columns
sampled_points@data[["unique_id"]] <- sampled_points@data[[tdat_idvar]]
sampled_points@data[["wgtcat"]] <- sampled_points@data[[wgtcats_var]]
sampled_points@data[["wgtcat"]] <- as.character(sampled_points@data[["wgtcat"]])
sampled_points@data$Reporting.Unit <- output_filename

```


### Filter sampled points
```{r filter sampled points, message=FALSE}
# double check and remove targeted points
summary(as.factor(sampled_points$Design))
# theres a bunch of NAs I need to cross reference with the SDD
# for now well label them as unknown
sampled_points$Design[is.na(sampled_points$Design)] <- "Unknown"

sampled_points <- sampled_points[sampled_points$Design != "Random Targeted" & sampled_points$Design != "Targeted", ]

# theres a couple points here that were sampled miles away from the desing point - well remove those also
targeted <- c("18060411490727722018-09-01","CO_SLVFO_LUP_2020_HILL_011_V22022-09-01")

sampled_points <- sampled_points[!sampled_points$PrimaryKey %in% targeted,]

# double check sampling order and design holes
# not applicable here since we are looking at just a subset of the design
# looking at this by project and plot id which is not the best but gets at holes within a year
# project_list <- list()
# 
# for(project in sampled_points$ProjectName){
#   # find the number in plotid
#   data <- sampled_points[sampled_points$ProjectName == project, c("PlotID", "DateVisited")]
#   data$plot_order <- regmatches(data$PlotID, regexpr("[[:digit:]]+",data$PlotID))
#   project_list[project] <- data
#   # check to see if higher numbers were sampled sequentially
# }

# remove points that didnt fall in a benchmark group - well also need to double check these polys are removed from the inference area
# we can check this by looking for nulls in benchmark fields
benchmark_fields <- unique(benchmarks$Objective.name)

for(field in benchmark_fields){
  sampled_points <- sampled_points[!is.na(sampled_points@data[[field]]),]
}

### Combine sampled points with rejections
# add benchmark fields to rejections
for(col in benchmark_fields){
  design_points[[col]] <- NA
}

all_points <- rbind(design_points[, c("unique_id", "fate", "wgtcat", benchmark_fields)], 
                    sampled_points[, c("unique_id", "fate", "wgtcat", benchmark_fields)])

# there one rejection that doesnt have a specific criteria
# it should be Inaccessbile
all_points$fate[all_points$fate == "Rejected"] <- "Inaccessible"
# theres an issue with the weight gen function below so updating to shortern versions of rejection 
all_points$fate[all_points$fate == "Inaccessible"] <- "IA"
all_points$fate[all_points$fate == "Non-Target"] <- "NT"
```

## Weighting
```{r point weighting, message=FALSE, warning=FALSE}
###### Analyze ######
weight_info <- weight.gen(pts = all_points,# remove targeted plots
                          pts.fatefield = "fate",
                          pts.groupfield = "wgtcat",
                          frame.spdf = wgtcat_spdf,
                          frame.groupfield = "wgtcat",
                          target.values = c("Target Sampled", "TARGET SAMPLED", "TS"),
                          unknown.values = c("Unknown", "UNKNOWN", "UNK"),
                          nontarget.values = c("NT"),
                          inaccessible.values = c("IA"),
                          unneeded.values = invalid_fates)

point_weights <-  weight_info[["point.weights"]]

all_points <-  aim.analysis::add_coords(all_points,
                                        xynames = c("x", "y"))
```

### Organise Indicators
```{r}
# make indicators long instead of wide
benchmarked_points <- all_points@data %>% 
  pivot_longer(cols = benchmark_fields, names_to = 'Indicator', values_to = "Condition.Category")

# remove the unsamplEd plots
benchmarked_points <- benchmarked_points[!is.na(benchmarked_points$Condition.Category),]

#Add reporting unit
all_points[["Reporting.Unit"]] <- output_filename
weight_info[["point.weights"]][["Reporting.Unit"]] <- output_filename
# need to add reporting unit name to analyse
point_weights$Reporting.Unit <- output_filename
```

### Category Definitions
```{r prep for analyze, message=FALSE}
# a definitions list for analyze_cat_multi()
# this is just a list of all the possible condition categorys
cat_defs <-  benchmarks[,c("Benchmark.Group", "Objective.name", "Condition.Category")]

colnames(cat_defs) <- c("Benchmark.Group","Indicator", "Condition.Category")
cat_defs <- unique(cat_defs)
```

## Analyse
```{r analyze, message=FALSE}
analysis <- analyze_cat_multi(data = benchmarked_points,
                        weights = point_weights,
                        id_var = "unique_id",
                        cat_var = "Condition.Category",
                        wgt_var = "WGT",
                        split_vars = "Indicator",
                        definitions = cat_defs,
                        conf = confidence,
                        verbose = FALSE)
```

### Confidence Intervals
```{r confidence intervals, message=FALSE}
# Calculate CIs for acres
total_acres = (analysis$total_observation_weight/analysis$weighted_observation_proportion)[1]

analysis <- analysis %>% 
  mutate(lower_bound_acres = weighted_observation_proportion_lower_bound_80pct*total_acres,
         upper_bound_acres = weighted_observation_proportion_upper_bound_80pct*total_acres)

analysis$Reporting.Unit <- output_filename

names(analysis) <- c("Rating",
                     "Number of plots",
                     "Unweighted proportion of sampled area",
                     "Estimated hectares",
                     "Weighted proportion of sampled area",
                     paste0(c("Lower confidence bound of percent area (", "Upper confidence bound of percent area ("), confidence, "%, Goodman multinomial) "),
                     "Indicator",
                     paste0(c("Lower confidence bound of acres (", "Upper confidence bound of acres ("), confidence, "%, Goodman multinomial)"),
                     "Reporting Unit")

```

### Formatting Outputs
```{r formatting outputs, message=FALSE}
wgtcat_summary <- weight_info[["frame.summary"]]
names(wgtcat_summary) <- c("wgtcat",
                           "total_count",
                           "count_Target Sampled",
                           "count_NT",
                           "count_IA",
                           "count_NA",
                           "count_UNK",
                           "area_ha",
                           "observed_proportion",
                           "observed_area",
                           "weight")

fates <- c("NT","IA","NA","UNK")

for (fate in fates) {
  wgtcat_summary[[paste0("count_", fate)]][is.na(wgtcat_summary[[paste0("count_", fate)]])] <- 0
}

wgtcat_summary[["in_inference"]] <- (wgtcat_summary[["weight"]])>0

wgtcat_summary$area_units <- "hectares"

wgtcat_summary$unobserved_area <- wgtcat_summary$area_ha - wgtcat_summary$observed_area

# Add in where they're from
all_points <- sp::merge(x = all_points,
                        y = point_weights[,c("unique_id","WGT")],
                        all.x = TRUE,
                        by = "unique_id")

# adding a field with sum of benchmarks meeting for all benchmarks and each individual standard
standard1_fields = colnames(all_points@data)[str_detect(colnames(all_points@data), "Standard1")]
standard3_fields = colnames(all_points@data)[str_detect(colnames(all_points@data), "Standard3")]

num_benchmarks <- length(benchmark_fields)
num_benchmarks_std1 <- length(standard1_fields)
num_benchmarks_std3 <- length(standard3_fields)

all_points@data <- all_points@data %>%
  mutate(sum_meeting = num_benchmarks - rowSums(across(.cols = all_of(benchmark_fields), .fns = str_detect, "Not Meeting")),
         sum_meeting_std1 = num_benchmarks_std1 - rowSums(across(.cols = all_of(standard1_fields), .fns = str_detect, "Not Meeting")),
         sum_meeting_std3 = num_benchmarks_std3 - rowSums(across(.cols = all_of(standard3_fields), .fns = str_detect, "Not Meeting")),
         percent_meeting = sum_meeting/num_benchmarks*100,
         percent_meeting_std1 = sum_meeting_std1/num_benchmarks_std1*100,
         percent_meeting_std3 = sum_meeting_std3/num_benchmarks_std3*100,
         benchmark_index = NA)


# reformat benchmark_points to wide format
wgtcat_spdf <- merge(x = wgtcat_spdf,
                     y = wgtcat_summary,
                     by = "wgtcat")

# Rounding numbers for clarity
wgtcat_spdf@data[,is.numeric(wgtcat_spdf@data)] <- round(is.numeric(wgtcat_spdf@data), 2)
```
## Writing Outputs
```{r writing outputs, message=FALSE, warning=FALSE}
write.csv(point_weights,
          file = paste0(output_path, "/",
                        output_filename,
                        "_pointweights_",
                        date, ".csv"),
          row.names = FALSE)
write.csv(wgtcat_summary,
          file = paste0(output_path, "/",
                        output_filename,
                        "_wgtcatsummary_",
                        date, ".csv"),
          row.names = FALSE)
write.csv(analysis,
          file = paste0(output_path, "/",
                        output_filename,
                        "_analysis_",
                        date, ".csv"),
          row.names = FALSE)
rgdal::writeOGR(all_points,
                dsn = output_path,
                layer = paste0(output_filename,
                               "_points_",
                               date),
                driver = "ESRI Shapefile",
                overwrite_layer = TRUE)

rgdal::writeOGR(wgtcat_spdf,
                dsn = output_path,
                layer = paste0(output_filename,
                               "_wgtcats_",
                               date),
                driver = "ESRI Shapefile",
                overwrite_layer = TRUE)

# lets also write these to gdb to avoid horrible truncated field names...
library(arcgisbinding)
arc.check_product() 

arc.write(data = all_points,
          path = paste0(output_path,"/",
                        analysis_gdb,"/",output_filename,
                               "_points"),
          overwrite = TRUE)

arc.write(data = wgtcat_spdf,
          path = paste0(output_path,"/",
                        analysis_gdb,"/",output_filename,
                               "_wgtcats"),
          overwrite = TRUE)
```
## Results
### Benchmarks
```{r, message=FALSE}
benchmarks_table <- benchmarks %>% DT::datatable(extensions = 'Buttons', filter = "top" , 
                            options = list(scrollX = TRUE ,
                            dom = 'Bfrtip',
                            buttons =
                            list(list(
                            extend = 'collection',
                            buttons = c('csv', 'excel'),
                            text = 'Download Table'))) , 
                 caption = paste("Weighted Analysis Benchmarks for " , 
                                 toString(output_filename)) , 
                 rownames = FALSE)
benchmarks_table
```

### Point Weights
```{r, message=FALSE}
weights_table <- point_weights %>% DT::datatable(extensions = 'Buttons', filter = "top" , 
                            options = list(scrollX = TRUE ,
                            dom = 'Bfrtip',
                            buttons =
                            list(list(
                            extend = 'collection',
                            buttons = c('csv', 'excel'),
                            text = 'Download Table'))) , 
                 caption = paste("Point Weights for " , 
                                 toString(output_filename)) , 
                 rownames = FALSE)
weights_table
```
### Figures

#### Proportional Estimates Tables
```{r, fig.width= 12, fig.height=8, message=FALSE}
# use dt table to present analysis and wgtcat summary
library(DT)

# round numeric
analysis <- analysis %>%
  mutate_if(is.numeric, round, 2)

analysis_table <- analysis %>%
  DT::datatable(extensions = 'Buttons', filter = "top" , 
                            options = list(scrollX = TRUE ,
                            dom = 'Bfrtip',
                            buttons =
                            list(list(
                            extend = 'collection',
                            buttons = c('csv', 'excel'),
                            text = 'Download Table'))) , 
                 caption = paste("Weighted Analysis Results in " , 
                                 toString(output_filename)) , 
                 rownames = FALSE)

analysis_table

```

#### Weight Category Summaries
```{r, fig.width= 12, fig.height=8, message=FALSE, warning=FALSE}
wgtcat_table <- wgtcat_summary %>%
  DT::datatable(extensions = 'Buttons', filter = "top" , 
                            options = list(scrollX = TRUE ,
                            dom = 'Bfrtip',
                            buttons =
                            list(list(
                            extend = 'collection',
                            buttons = c('csv', 'excel'),
                            text = 'Download Table'))) , 
                 caption = paste("Weight Category Summary in  " , 
                                 toString(output_filename)) , 
                 rownames = FALSE)
wgtcat_table
```

#### Plotting weighted analysis results
```{r, fig.width=12, fig.height=8, message=FALSE}
# Fancy palette from Nelson
library(scales)
palette = c("#f5bb57ff",
                                            "#f8674cff",
                                            "#4a8b9fff",
                                            "#685b7fff",
                                            "#c95294ff",
                                            "#f5a9c6ff",
                                            "#75c6c5ff",
                                            "#fd6794ff")

analysis$Rating <- as.factor(analysis$Rating)

levels(analysis$Rating)

analysis$Rating <- factor(analysis$Rating,levels(factor(analysis$Rating))[c(2,3,4,1,5)])

# Join in nice looking name

analysis_merge <- merge(x = analysis,
                  y = unique(benchmarks[,c("Objective.name", "Management.Question")]),
                  by.x = "Indicator",
                  by.y = "Objective.name",
                  all.x = TRUE,
                  all.y = FALSE)

g <- ggplot(data = analysis_merge,
                aes(y = `Estimated hectares`,
                             x = Rating, fill = Rating)) +
  geom_col(alpha = 0.6, position = "dodge") +
  geom_errorbar(aes(ymin = `Lower confidence bound of acres (80%, Goodman multinomial)`,
                                      ymax = `Upper confidence bound of acres (80%, Goodman multinomial)`),
                         width = 0.5)+
  facet_wrap(.~Management.Question, scales = "free")+
  coord_flip()+
  theme_bw()+
  labs(x = "")+
  scale_fill_manual(values = palette)+
  scale_y_continuous(label = comma)+
  theme(panel.spacing.y = unit(2,"lines"))

ggplotly(g)

# lets group by LHS and put all indicators on the same graph
# and switch o points instead of cols

# weights from the continuous analysis
LHS1 <- analysis_merge[analysis_merge$Indicator %in% standard1_fields,] %>% 
  mutate(`Estimated Acres` = `Estimated hectares`* 2.471,
         `Lower confidence bound of acres (80%, Goodman multinomial)` = `Lower confidence bound of acres (80%, Goodman multinomial)` * 2.471,
         `Upper confidence bound of acres (80%, Goodman multinomial)` = `Upper confidence bound of acres (80%, Goodman multinomial)` * 2.471,
         `Standard 1 Indicator` = sub("SLVFO Standard 1 ", "", Management.Question)) %>% 
  select(-c(`Estimated hectares`)) %>% 
  ggplot(aes(y = `Estimated Acres`,
             x = `Standard 1 Indicator`, col = Rating))+
  geom_point(position = position_dodge(width = 0.5))+
  geom_errorbar(aes(ymin = `Lower confidence bound of acres (80%, Goodman multinomial)`,
                                      ymax = `Upper confidence bound of acres (80%, Goodman multinomial)`),
                         width = 0.5,position = position_dodge(width = 0.5))+
  theme_bw()+
  ggtitle(label = "Standard 1 Analysis Results for Alamosa-trinchera")+
  theme(axis.text.x = element_text(angle = 40, hjust = 1))+
  geom_hline(aes(yintercept= 178759.2),linetype = 2, colour = "blue" )+
  scale_y_continuous(labels = comma_format())

LHS1

LHS3 <- analysis_merge[analysis_merge$Indicator %in% standard3_fields,] %>% 
  mutate(`Estimated Acres` = `Estimated hectares`* 2.471,
         `Lower confidence bound of acres (80%, Goodman multinomial)` = `Lower confidence bound of acres (80%, Goodman multinomial)` * 2.471,
         `Upper confidence bound of acres (80%, Goodman multinomial)` = `Upper confidence bound of acres (80%, Goodman multinomial)` * 2.471,
         `Standard 1 Indicator` = sub("SLVFO Standard 3 ", "", Management.Question)) %>% 
  select(-c(`Estimated hectares`)) %>% 
  ggplot(aes(y = `Estimated Acres`,
             x = `Standard 1 Indicator`, col = Rating))+
  geom_point(position = position_dodge(width = 0.5))+
  geom_errorbar(aes(ymin = `Lower confidence bound of acres (80%, Goodman multinomial)`,
                                      ymax = `Upper confidence bound of acres (80%, Goodman multinomial)`),
                         width = 0.5,position = position_dodge(width = 0.5))+
  theme_bw()+
  ggtitle(label = "Standard 3 Analysis Results for Alamosa-trinchera")+
  theme(axis.text.x = element_text(angle = 40, hjust = 1))+
  geom_hline(aes(yintercept= 178759.2),linetype = 2, colour = "blue" )+
  scale_y_continuous(labels = comma_format())


LHS3

ggsave(plot = g, device = "jpeg", dpi = 400, width =20, height = 10, filename = paste0("analysis_", output_filename, ".jpeg"), path = output_path)

ggsave(plot =LHS1, device = "jpeg", dpi = 400, width =8, height = 6, filename = paste0("LHS1_analysis_", output_filename, ".jpeg"), path = output_path)

ggsave(plot =LHS3, device = "jpeg", dpi = 400, width =8, height = 6, filename = paste0("LHS3_analysis_", output_filename, ".jpeg"), path = output_path)

```


