---
title: "Owyhee Weeds Weighted Analysis"
author: "Alex Traynor - modified by Jennifer E. Nelson"
date: "2022-12-28"
output: html_document
---

## Description

* Request initiated by Nelson, Jennifer E <jenelson@blm.gov>; for estimating number of infested acres of weeds in the Owyhee Field office.
* Benchmarks were provided by the FO in their MDW as well as work from Jennifer documented here:https://doimspp.sharepoint.com/:f:/r/sites/ext-blm-oc-naim/Shared%20Documents/AIM%20Projects/Terrestrial%20Projects/Idaho/Owyhee%20FO/Analysis?csf=1&web=1&e=YjcV6E. Additional benchmarks for invasive annual grass infestation and noxious weed cover were applied as follows: "Infested" if a plot detected >5% of BRTE, VEDU, TACA8, and Jointed Goatgrass combined cover, "Infested" if a plot detected >5% cover of any of the previous species listed, individually. "Infested" if a plot detected >1% cover of noxious species.
* Benchmark groups are based on Ecological Site groups based on groups of Ecological Site Descriptions identified in the field by AIM crews. Plots without field based ESD identification were assigned Ecological Site group based on spatial location. A look up table relating ESD to Ecological Site groups can be found at the above sharepoint site.
* Analysis includes both AIM and LMF plots. Only AIM plots from the 2016 LUP design were inlcuded. One of these plots was removed (BigWD-104) because it was sampled 325 meters from its design location rendering it effecively a targeted plot.
* The analysis area also includes 6 paired plots located using a targeted design for the Pedro fire. These are located on Bureau of Reclamation land. These will be used in a point counting analysis alongside the weighted estimates.

## Setup
```{r setup, message=FALSE}
### Package Setup
# Set local library path and load required packages
library(sp)
library(dplyr)
library(stringr)
library(sf)
library(rgdal)
library(tidyr)
#library(tidyverse)
#library(spsurvey)
```

### Set variables and input parameters

```{r input variables, message=FALSE}
#Alber's Equal Area is useful for calculating areas
projection <- sp::CRS("+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")

#Analysis date
date <- Sys.Date()

# Confidence level in percent
confidence <- 80

output_path <- "C:\\Users\\alaurencetraynor\\Documents\\ID\\OFO_Weeds_WA\\outputs"

# Filename for benchmark tool from analysis requester
benchmarks_filename <- NULL

# Core filename for all outputs/ name of reporting unit
output_filename <- "OFO_Weeds"

# The variable in AIM sample designs that contains the unique identifier for each point
aim_idvar <- "PlotID" # since this is a single design plot id is unique

# The variable in AIM sample designs that contains the fate for each point
aim_fatevar = "PlotStatus"

# The variable in TerrADat that contains the unique identifier for each point matching AIM designs'
tdat_idvar <- "PlotID"

# The fates in sample designs which correspond to sampled/observed
observed_fates = c("TS", "Target Sampled", "Sampled")

# The fates in sample designs which correspond to unneeded
# (e.g. unused oversample or points designated for future sampling)
invalid_fates = c("NN", NA, "Non Needed", "", "Not Sampled")

# sdd points
design_points_name <- "REJ_OFO"

# terradat clipped points
# this includes both aim and lmf points
sampled_points_name <- "Benchmarked_points_combined"

# The variable that contains the areas in HECTARES in wgtcats
wgtcat_area_var = "gis_acres"

# Reporting unit polygon/layer
wgtcats_name <- "lmf_lup_strata_Union_Dissolv1_aea"

# Geodatabase containing all spatial files
analysis_gdb <- "OFO_Weeds_WA.gdb"

# The unique ID for poststratification polygons

wgtcats_var <- "wgtcat"

### The filepath to the folder where we have all the spatial data
path_spatial <- "C:\\Users\\alaurencetraynor\\Documents\\ID\\OFO_Weeds_WA\\Inputs"

### The filepath to the folder where we have the Excel stuff
path_tabular <- "C:\\Users\\alaurencetraynor\\Documents\\ID\\OFO_Weeds_WA\\Inputs"

#my favourite function
`%notin%` <- Negate(`%in%`)

#benchmark group var in design points
benchmark_var <- "BenchmarkGroup"

# targeted points we'll remove this before weighting but after benchmarking
targeted_plots <- c("BigWD-104")
```

## Import Spatial and Tabular Data
### Read in weight categories
```{r read strata, message=FALSE, warning=FALSE}
#### Strata polygons
wgtcat_df <-sf::st_read(dsn = paste(path_spatial, analysis_gdb, sep = "/"),
                        layer = wgtcats_name,
                        stringsAsFactors = FALSE)
## Standardize projection
wgtcat_df <- sf::st_transform(wgtcat_df,
                              crs = projection)

## Make spatial 
wgtcat_spdf <- methods::as(wgtcat_df, "Spatial")
wgtcat_spdf@data[["wgtcat"]] <- wgtcat_spdf@data[[wgtcats_var]]
sf::st_geometry(wgtcat_df) <- NULL

wgtcat_df[["wgtcat"]] <- wgtcat_df[[wgtcats_var]]
wgtcats <- as.data.frame(wgtcat_df)
```

### Read design points
```{r read design points, message=FALSE, warning=FALSE}
#### Read in design points
# Make sure points and polygons do not include Z/M dimensions...
design_points <- sf::st_read(dsn = paste(path_spatial, analysis_gdb, sep = "/"),
                             layer = design_points_name,
                             stringsAsFactors = FALSE)
## Standardize projection
design_points <- sf::st_transform(design_points,
                                  crs = projection)
# theres a weird NA in here
# drop it
design_points <- design_points[!is.na(design_points$PlotID),]

## Make spatial 
design_points <- methods::as(design_points, "Spatial")

# Standardise columns
design_points@data[["unique_id"]] <- design_points@data[[aim_idvar]]
design_points@data[["fate"]] <- design_points@data[[aim_fatevar]]
design_points@data[["wgtcat"]] <- design_points@data[[wgtcats_var]]
design_points@data[["Benchmark.Group"]] <- design_points@data[[benchmark_var]]

```

### Read Sampled points
#### These have already been benchmarked from the benchmark tool
```{r sampled points, message=FALSE, warning=FALSE}
#### Read in Terradat points
# Make sure points and polygons do not include Z/M dimensions...
sampled_points <- sf::st_read(dsn = paste(path_spatial, analysis_gdb, sep = "/"),
                              layer = sampled_points_name,
                              stringsAsFactors = FALSE)
## Standardize projection
sampled_points <- sf::st_transform(sampled_points,
                                   crs = projection)
## Make spatial 
sampled_points <- methods::as(sampled_points, "Spatial")

sampled_points@data[["fate"]] <- "TS"

# Standardise columns
sampled_points@data[["unique_id"]] <- sampled_points@data[[tdat_idvar]]
sampled_points@data[["wgtcat"]] <- sampled_points@data[[wgtcats_var]]
sampled_points@data[["wgtcat"]] <- as.character(sampled_points@data[["wgtcat"]])
sampled_points@data$Reporting.Unit <- output_filename

sampled_points_benchmarked <- sampled_points
```

### Point QC
```{r point checks, message=FALSE}
# Checking against the ratings confirms if all the rated plots are represented (important)
# and if all the sampled_plots are rated (not important, but informative)
if (!all(sampled_points_benchmarked[["unique_id"]] %in% sampled_points$unique_id)) {
  sampled_points_benchmarked$unique_id[sampled_points_benchmarked[["unique_id"]] %notin% sampled_points$unique_id]
  warning("NOT ALL RATED AIM PLOTS ARE PRESENT IN sampled_points!")
} else (print("All rated AIM plots are present in sampled_points"))

if (!all(sampled_points$unique_id %in% sampled_points_benchmarked[["unique_id"]])){
  warning("NOT ALL SAMPLED AIM PLOTS ARE RATED!")
} else(print("All sampled AIM plots have been rated"))

```
## Filter sampled points
```{r filter sampled points, message=FALSE}
# filter sampled plots by those that were rated:
# sampled_points <- sampled_points[sampled_points$unique_id %in% sampled_points_benchmarked[["Plot_ID"]], ] 

# we also want to filter out the targeted plots here and plots not in sagebrush
# there arent any rejected points so we can just filter using the benchmarked points
sampled_points <- sampled_points[sampled_points$unique_id %in% sampled_points_benchmarked$unique_id,]
sampled_points$area_ratio <- NA
sampled_points$Source<- NULL
for(i in 1:nrow(sampled_points)){
  if(sampled_points$ProjectName[i]=="LMF"){
    sampled_points$Source[i]<-"LMF"
  } else {
    sampled_points$Source[i]<-"AIM"
  }
}

# there are no desing points intersecting with LMF segments but need to add area_ratio field
design_points$area_ratio <- NA
design_points$Source <- "AIM"

### Replace all sampled points with their TerrADat counterparts
all_points <- rbind(design_points[!(design_points$fate %in% observed_fates), c("unique_id", "fate", "wgtcat", "Source","segment_id", "area_ratio")], 
                    sampled_points[, c("unique_id", "fate", "wgtcat", "Source","segment_id", "area_ratio")])

```

### Weighting
```{r point weighting, message=FALSE, warning=FALSE, eval=FALSE}
# These are being redefined here as though they were arguments to the function
aim_points = all_points[all_points$Source == "AIM",]
lmf_points = all_points[all_points$Source == "LMF",]
aim_idvar <- "unique_id"
lmf_idvar <- "unique_id"
aim_fatevar <- "fate"
wgtcat_var <- "wgtcat"
segments <- NULL
segment_var <- "segment_id"
verbose <- FALSE

if (length(wgtcat_var) != 1 | class(wgtcat_var) != "character") {
  stop("wgtcat_var must be a single character string")
}
if (!is.null(wgtcat_area_var)) {
  if (length(wgtcat_area_var) != 1 | class(wgtcat_area_var) != "character") {
    stop("wgtcat_area_var must be a single character string")
  }
}
if (length(segment_var) != 1 | class(segment_var) != "character") {
  stop("segment_var must be a single character string")
}

if (!is.null(wgtcats)) {
  if (!(class(wgtcats) %in% c("SpatialPolygonsDataFrame", "data.frame"))) {
    stop("wgtcats must be a spatial polygons data frame or data frame")
  }
  if (nrow(wgtcats) < 1) {
    stop("wgtcats contains no observations/data")
  }
  if (!(wgtcat_var %in% names(wgtcats))) {
    stop(paste("The variable", wgtcat_var, "does not appear in wgtcats@data"))
  }
}

if (!is.null(segments)) {
  if (!(class(segments) %in% "SpatialPolygonsDataFrame")) {
    stop("segments must be a spatial polygons data frame")
  }
  if (nrow(segments) < 1) {
    stop("segments contains no observations/data")
  }
  if (!(segment_var %in% names(segments))) {
    stop(paste("The variable", segment_var, "does not appear in segments@data"))
  }
}

if (is.null(aim_fatevar)) {
  warning("No fate variable specified for AIM points. Assuming all were observed/sampled.")
  aim_fatevar <- "fate"
  aim_points[["fate"]] <- "observed"
  lmf_points[["fate"]] <- "observed"
  observed_fates <- "observed"
} else {
  if (length(aim_fatevar) > 1 | class(aim_fatevar) != "character") {
    stop("The aim fate variable must be a single character string")
  }
  if (!aim_fatevar %in% names(aim_points)) {
    stop(paste("The variable", aim_fatevar, "does not appear in aim_points@data"))
  } else {
    aim_points[["fate"]] <- aim_points[[aim_fatevar]]
  }
  if (is.null(observed_fates)) {
    warning("No observed fates provided. Assuming all AIM points were observed/sampled unless specified otherwise with invalid_fates")
    observed_fates <- unique(aim_points[["fate"]])
    observed_fates <- observed_fates[!(observed_fates %in% invalid_fates)]
  }
}

lmf_points[["fate"]] <- observed_fates[1]

if (!is.null(observed_fates)) {
  if (!any(aim_points[["fate"]] %in% observed_fates)) {
    warning("No AIM points have a fate specified as observed.")
  }
}

# Harmonize projections
if (is.null(projection)) {
  if (!is.null(wgtcats)) {
    projection <- wgtcats@proj4string
  } else if (!is.null(segments)) {
    projection <- segments@proj4string
  }
}

if (!is.null(projection)) {
  if (class(aim_points) %in% c("SpatialPointsDataFrame")) {
    if (!identical(projection, aim_points@proj4string)) {
      aim_points <- sp::spTransform(aim_points,
                                    CRSobj = projection)
    }
  }
  if (class(lmf_points) %in% c("SpatialPointsDataFrame")) {
    if (!identical(projection, lmf_points@proj4string)) {
      lmf_points <- sp::spTransform(lmf_points,
                                    CRSobj = projection)
    }
  }
  if (!is.null(wgtcats)) {
    if (class(lmf_points) %in% c("SpatialPolygonsDataFrame")) {
      if (!identical(projection, wgtcats)) {
        wgtcats <- sp::spTransform(wgtcats,
                                   CRSobj = projection)
      }
    }
  }
  if (!is.null(segments)) {
    if (!identical(projection, segments@proj4string)) {
      segments <- sp::spTransform(segments,
                                  CRSobj = projection)
    }
  }
}



# Assign the weight categories
if (class(wgtcats) %in% "SpatialPolygonsDataFrame") {
  if (!(wgtcat_var %in% names(wgtcats))) {
    stop("The variable ", wgtcat_var, " does not appear in wgtcats")
  }
  aim_points[["wgtcat"]] <- sp::over(aim_points, wgtcats)[[wgtcat_var]]
  lmf_points[["wgtcat"]] <- sp::over(lmf_points, wgtcats)[[wgtcat_var]]
} else {
  if (!(wgtcat_var %in% names(aim_points))) {
    stop("The variable ", wgtcat_var, " does not appear in aim_points")
  }
  if (!(wgtcat_var %in% names(lmf_points))) {
    stop("The variable ", wgtcat_var, " does not appear in lmf_points")
  }
  aim_points[["wgtcat"]] <- aim_points[[wgtcat_var]]
  lmf_points[["wgtcat"]] <- lmf_points[[wgtcat_var]]
}


# Assign the LMF segment codes
if (is.null(segments)) {
  if (!(segment_var %in% names(aim_points))) {
    stop("The variable ", segment_var, " does not appear in aim_points")
  }
  if (!(segment_var %in% names(lmf_points))) {
    stop("The variable ", segment_var, " does not appear in lmf_points")
  }
  aim_points[["segment"]] <- aim_points[[segment_var]]
  lmf_points[["segment"]] <- lmf_points[[segment_var]]
} else {
  if (!(segment_var %in% names(segments))) {
    stop("The variable ", segment_var, " does not appear in segments")
  }
  aim_points[["segment"]] <- sp::over(aim_points, segments)[[segment_var]]
  lmf_points[["segment"]] <- sp::over(lmf_points, segments)[[segment_var]]
}

# Just harmonize the idvar names for now
aim_points[["unique_id"]] <- aim_points[[aim_idvar]]
lmf_points[["unique_id"]] <- lmf_points[[lmf_idvar]]

# Add reporting units
aim_points[["reporting_unit"]] <- output_filename
lmf_points[["reporting_unit"]] <- output_filename

# If somehow LMF points aren't in a segment, that's a major problem
if (any(is.na(lmf_points[["segment"]]))) {
  stop(paste("The following LMF points did not spatially intersect any segment polygons:",
             paste(lmf_points[is.na(lmf_points[["segment"]]), "unique_id"], collapse = ", ")))
}

# TODO: Stick a check in here that the segment ID from the polygons
# matches the one derived from the LMF plot ID
# Probably just warn if not?


# Get data frames
if (class(aim_points) %in% "SpatialPointsDataFrame") {
  aim_df <- aim_points@data
} else {
  aim_df <- aim_points
}
if (class(lmf_points) %in% "SpatialPointsDataFrame") {
  lmf_df <- lmf_points@data
} else {
  lmf_df <- lmf_points
}

# NOTE THAT THIS FILTERS OUT ANYTHING FLAGGED AS NOT NEEDED IN THE FATE
# So that'd be unused oversamples or points from the FUTURE that no one would've sampled anyway
aim_df <- aim_df[!(aim_df[["fate"]] %in% invalid_fates), c("unique_id", "fate", "wgtcat", "segment", "area_ratio")]
aim_df[["aim"]] <- TRUE
aim_df[["lmf"]] <- FALSE

# We only have target sampled LMF points available to us, so we don't need to filter them
lmf_df <- lmf_df[, c("unique_id", "fate", "wgtcat", "segment", "area_ratio")]
lmf_df[["aim"]] <- FALSE
lmf_df[["lmf"]] <- TRUE

# Combine them
combined_df <- unique(rbind(aim_df, lmf_df))

# There shouldn't be any that don't belong to a wgtcat anymore
combined_df <- combined_df[!is.na(combined_df[["wgtcat"]]), ]

# Add an observed variable for easy reference later
combined_df[["observed"]] <- combined_df[["fate"]] %in% observed_fates

# To make the lookup table, drop any points that fell outside LMF segments
combined_segmentsonly_df <- combined_df[!is.na(combined_df[["segment"]]), ]

# Create a segment relative weight lookup table
segment_relwgt_lut <- do.call(rbind,
                              lapply(X = split(combined_segmentsonly_df, combined_segmentsonly_df[["segment"]]),
                                     FUN = function(X){
                                       # These are the count of AIM points with any valid fate
                                       aim_count <- sum(X[["aim"]])
                                       
                                       # We also need the count of LMF points with any valid fate, but that's complicated
                                       # We only have the sampled LMF points so we can count those
                                       lmf_sampled_count <- sum(X[["lmf"]])
                                       # To get the number of evaluated but not sampled points:
                                       # The LMF plot keys end in a digit that represents the intended sampling order within a segment
                                       # 1 and 2 are considered base points and were intended to be sampled
                                       # If a sampled LMF plot's plot key ends in 3, that means that one or both of the base points
                                       # were evaluated and rejected rather than sampled, which brings the evaluated LMF plot count
                                       # to three for the segment.
                                       # This just asks if the third point was used
                                       lmf_oversample_used <- any(grepl(X[["unique_id"]][X[["lmf"]]],
                                                                        pattern = "\\D3$"))
                                       
                                       # Likewise, if only one LMF plot was sampled in a segment, that means the other two were
                                       # evalurated and rejected rather than sampled, also bringing the total to three.
                                       # So if there was only one sampled or if the oversample was used, there were three evaluated
                                       if (sum(X[["lmf"]]) == 1 | lmf_oversample_used) {
                                         lmf_count <- 3
                                       } else {
                                         # This will fire only if there sampled count was 2, but better to be safe here
                                         lmf_count <- lmf_sampled_count
                                       }
                                       
                                       
                                       # The relative weight for points falling within a segment is calculated as
                                       # 1 / (number of points)
                                       relative_weight <- 1 / sum(aim_count, lmf_count)
                                       
                                       output <- data.frame("segment" = X[["segment"]][1],
                                                            "relwgt" = relative_weight,
                                                            stringsAsFactors = FALSE)
                                       return(output)
                                     }))

# Add the relative weights to the combined AIM and LMF points
combined_df <- merge(x = combined_df,
                     y = segment_relwgt_lut,
                     by = "segment",
                     all.x = TRUE)

# adjust relwgt based on area of lmf segment:
combined_df <- combined_df %>% mutate(area_ratio = round(area_ratio, digits = 3)) %>%
                               mutate(relwgt = relwgt * area_ratio)

# Anywhere there's an NA associated with an AIM point, that's just one that fell outside the segments
combined_df[is.na(combined_df[["relwgt"]]) & combined_df[["aim"]], "relwgt"] <- 1

if (is.null(wgtcat_area_var)) {
  if (class(wgtcats) %in% "SpatialPolygonsDataFrame") {
    wgtcat_df <- aim.analysis::add.area(wgtcats)@data
  } else {
    stop("No name for a variable in wgtcats containing the area in hectares was provided and wgtcats is not a spatial polygons data frame so area can't be calculated")
  }
} else {
  if (!(wgtcat_area_var %in% names(wgtcats))) {
    if (class(wgtcats) %in% "SpatialPolygonsDataFrame") {
      wgtcat_df <- aim.analysis::add.area(wgtcats)@data
    } else {
      stop("The variable ", wgtcat_area_var, " does not appear in wgtcats")
    }
  } else {
    warning("Trusting that the variable ", wgtcat_area_var, " in wgtcats contains the areas in hectares")
    if (class(wgtcats) %in% "SpatialPolygonsDataFrame") {
      wgtcat_df <- wgtcats@data
      wgtcat_df[["AREA.HA"]] <- wgtcat_df[[wgtcat_area_var]]
    } else {
      wgtcat_df <- wgtcats
      wgtcat_df[["AREA.HA"]] <- wgtcat_df[[wgtcat_area_var]]
    }
  }
}


wgtcat_df[["wgtcat"]] <- wgtcat_df[[wgtcat_var]]

# Making sure that these aren't spread out over multiple observations
if (verbose) {
  message("Summarizing wgtcat_df by wgtcat to calculate areas in case the wgtcats are split into multiple observations")
}
wgtcat_df <- dplyr::summarize(dplyr::group_by(wgtcat_df,
                                              wgtcat),
                              "hectares" = sum(AREA.HA))

wgtcat_areas <- setNames(wgtcat_df[["hectares"]], wgtcat_df[["wgtcat"]])

# had a issue here reading wgtcats as integers
wgtcat_df$wgtcat <-  as.character(wgtcat_df$wgtcat)
  
weight_info <- lapply(X = wgtcat_df[["wgtcat"]],
                      wgtcat_areas = wgtcat_areas,
                      points = combined_df,
                      wgtcat_var = wgtcat_var,
                      FUN = function(X, wgtcat_areas, points, wgtcat_var){
                        # Area of this polygon
                        area <- wgtcat_areas[X]
                        
                        # All of the points (AIM and LMF) falling in this polygon
                        points <- points[points[["wgtcat"]] == X, ]
                        
                        # If there are in fact points, do some weight calculations!
                        if (nrow(points) > 0 & any(points[["observed"]])) {
                          # The number of observed AIM points with a relative weight of 1
                          # So, not sharing an LMF segment with any LMF points
                          aim_standalone <- sum(combined_df[["aim"]] & combined_df[["relwgt"]] == 1)
                          # The number of unique segments selected for LMF points
                          lmf_segment_count <- length(unique(points[["segment"]]))
                          
                          # The approximate number of 160 acre segments in this polygon
                          # Obviously, not all of the segments were selected in the first stage
                          # of the LMF design, but this is how many were available in this polygon
                          approximate_segment_count <- area / (160 / 2.47)
                          
                          # This is the sum of the relative weights of the OBSERVED points
                          # This does not include the inaccessible, rejected, or unknown points
                          # It does include both AIM and LMF, however
                          sum_observed_relwgts <- sum(points[points[["observed"]], "relwgt"])
                          # This is the sum of the relative weights of all the AIM points, regardless of fate
                          sum_relwgts <- sum(points[["relwgt"]])
                          
                          # The units are segments per point
                          # The segments are 120 acre quads (quarter sections?) that the first stage of LMF picked from
                          # Steve Garman called this "ConWgt" which I've expanded to conditional_weight
                          # but that's just a guess at what "con" was short for
                          conditional_weight <- approximate_segment_count / (aim_standalone + lmf_segment_count)
                          conditional_area <- conditional_weight * sum_observed_relwgts
                          
                          # What's the observed proportion of the area?
                          observed_proportion <- sum_observed_relwgts / sum_relwgts
                          # And how many acres is that then?
                          # We can derive the "unknown" or "unsampled" area as the difference
                          # between the polygon area and the observed area
                          observed_area <- area * observed_proportion
                          
                          # Then this adjustment value is calculated
                          weight_adjustment <- observed_area / conditional_area
                          
                          
                          
                          # Put everything about the wgtcat in general in one output data frame
                          output_wgtcat <- data.frame(wgtcat = X,
                                                      area = area,
                                                      area_units = "hectares",
                                                      approximate_segment_count = approximate_segment_count,
                                                      sum_observed_relwgts = sum_observed_relwgts,
                                                      sum_relwgts = sum_relwgts,
                                                      observed_proportion = observed_proportion,
                                                      observed_area = observed_area,
                                                      unobserved_area = area - observed_area,
                                                      conditional_weight = conditional_weight,
                                                      conditional_area = conditional_area,
                                                      weight_adjustment = weight_adjustment,
                                                      point_count = sum(points[["observed"]]),
                                                      observed_point_count = nrow(points),
                                                      stringsAsFactors = FALSE)
                          
                          # But much more importantly add the calculated weights to the points
                          output_points <- points
                          
                          # This handles situations where there were points, but none of them were observed
                          # In that case, weight_adjustment will be 0 / 0 = NaN
                          # That's fine because we can identify that this polygon is still in the inference area
                          # but that its entire area is "unknown" because no data were in it
                          if (is.nan(weight_adjustment)) {
                            output_points[["wgt"]] <- NA
                          } else {
                            # The point weight is calculated here.
                            # This is Garman's formula and I don't have the documentation justifying it on hand
                            output_points[["wgt"]] <- weight_adjustment * conditional_weight * points[["relwgt"]]
                            message("Checking weight sum for ", X)
                            # I'm rounding here because at unrealistically high levels of precision it gets weird and can give false positives
                            if (round(sum(output_points[["wgt"]]), digits = 3) != round(area, digits = 3)) {
                              warning("The sum of the point weights (", sum(output_points[["wgt"]]), ") does not equal the polygon area (", area, ") for ", X)
                            }
                          }
                          
                        } else {
                          # Basically just empty data frames
                          output_wgtcat <- data.frame(wgtcat = X,
                                                      area = area,
                                                      area_units = "hectares",
                                                      approximate_segment_count = area / (160 / 2.47),
                                                      sum_observed_relwgts = NA,
                                                      sum_relwgts = NA,
                                                      observed_proportion = 0,
                                                      observed_area = 0,
                                                      unobserved_area = area,
                                                      conditional_weight = NA,
                                                      conditional_area = NA,
                                                      weight_adjustment = NA,
                                                      point_count = 0,
                                                      observed_point_count = 0,
                                                      stringsAsFactors = FALSE)
                          output_points <- NULL
                        }
                        
                        return(list(points = output_points,
                                    wgtcat = output_wgtcat))
                      })

point_weights <- do.call(rbind,
                         lapply(X = weight_info,
                                FUN = function(X){
                                  X[["points"]]
                                }))
wgtcat_summary <- do.call(rbind,
                          lapply(X = weight_info,
                                 FUN = function(X){
                                   X[["wgtcat"]]
                                 }))

```

### Analyse

```{r analyze, message=FALSE}
#Read in wgtcatsummary and pointweights
point_weights<-read.csv("C:\\Users\\alaurencetraynor\\Documents\\ID\\outputs\\OFO_LUP_pointweights_2022-12-01.csv")
wgtcat_summary<-read.csv("C:\\Users\\alaurencetraynor\\Documents\\ID\\outputs\\OFO_LUP_wgtcatsummary_2022-12-01.csv")

# need to tidy benchmark points 
benchmarked_points <- sampled_points_benchmarked@data %>% 
  pivot_longer(AnnualGrass_Infestation:NoxiousWeed_Occurrence, names_to = "Indicator", values_to = "Condition.Category") %>% 
  select(unique_id, Indicator, Condition.Category, BenchmarkGroup,  Latitude_NAD83, Longitude_NAD83)

benchmarked_points <- as.data.frame(benchmarked_points)
benchmarked_points<-benchmarked_points[!benchmarked_points$unique_id=="LowCM-255",]

# need to add reporting unit name to analyse
point_weights$Reporting.Unit <- output_filename

analysis <- analyze(benchmarked_points,
  point_weights,
  id_var = "unique_id",
  indicator_var = "Indicator",
  value_var = "Condition.Category",
  weight_var = "wgt",
  x_var = "Longitude_NAD83",
  y_var = "Latitude_NAD83",
  reporting_var = "Reporting.Unit",
  split_var = NULL,
  conf = 80)

### Formatting Outputs
analysis <- analysis[, c("Subpopulation",
                         "Indicator",
                         "Category",
                         "NResp",
                         "Estimate.P",
                         "Estimate.U")]

```

### Confidence Intervals
```{r confidence intervals, message=FALSE}
# adjusted counts = sum of all plots within each indicator  * estimated proportion
# adjusted count for a category = total observations * sum of weights of observations in the category / sum of all weights.
# remove totals first
analysis <- analysis[analysis$Category != "Total",]

# add in missing level
analysis[nrow(analysis) + 1,] = c("OFO_Weeds","JointedGoatGrass_Infestation", "Infested","0","0","0")
# sadly this converts everything to characters..
analysis$NResp <- as.numeric(analysis$NResp)
analysis$Estimate.P <- as.numeric(analysis$Estimate.P)
analysis$Estimate.U <- as.numeric(analysis$Estimate.U)

adjusted_counts <- lapply(X = split(analysis, analysis$Indicator), function(X){
  total_observations <- sum(X$NResp)
  X$weighted_observations <- (X$Estimate.P/100) * total_observations
  cis <- goodman_cis(counts = X$weighted_observations)
  return(cis)
  })

adjusted_counts <- do.call("rbind", adjusted_counts)
adjusted_counts$Indicator <- row.names(adjusted_counts)
adjusted_counts$proportion <- adjusted_counts$proportion *100

# remove the .1s
adjusted_counts$Indicator <- gsub("\\.[0-9]","", adjusted_counts$Indicator)

analysis_final <- analysis[order(analysis$Indicator, analysis$Estimate.P),]
adjusted_counts <- adjusted_counts[order(adjusted_counts$Indicator, adjusted_counts$proportion),]

adjusted_counts$Category <-  analysis_final$Category

analysis <- merge(x = analysis_final,
                  y = adjusted_counts[, c("lower_bound", "upper_bound", "Indicator", "Category")],
                  by = c("Indicator","Category"))

analysis[["lower_bound"]] <- analysis[["lower_bound"]] * 100
analysis[["upper_bound"]] <- analysis[["upper_bound"]] * 100

# Calculate the hectares using the Goodman binomial confidence intervals from percentage
total_hectares <- analysis[["Estimate.U"]]/(analysis[["Estimate.P"]]/100)
  
analysis[[paste0("LCB", confidence, "Pct.U.Goodman")]] <- analysis[["lower_bound"]] / 100 * total_hectares
analysis[[paste0("UCB", confidence, "Pct.U.Goodman")]] <- analysis[["upper_bound"]] / 100 * total_hectares

names(analysis) <- c("Rating",
                     "Indicator",
                     "Reporting Unit",
                     "Number of plots",
                     "Estimated percent of sampled area",
                     "Estimated hectares",
                     paste0(c("Lower confidence bound of percent area (", "Upper confidence bound of percent area ("), confidence, "%, Goodman multinomial) "),
                     paste0(c("Lower confidence bound of hectares (", "Upper confidence bound of hectares ("), confidence, "%, Goodman multinomial) "))

```

## Formatting Outputs
```{r formatting outputs, message=FALSE}

#wgtcat_summary[["in_inference"]] <- (wgtcat_summary[["observed_point_count"]])>0

# # Add in where they're from
# all_points <- sp::merge(x = all_points,
#                         y = point_weights[,c("unique_id","wgt")],
#                         all.x = TRUE,
#                         by = "unique_id")
# 
# all_points <- sp::merge(x = all_points,
#                         y = sampled_points_benchmarked,
#                         by = "unique_id",
#                         all.x = TRUE)
# 
# wgtcat_spdf <- merge(x = wgtcat_spdf,
#                      y = wgtcat_summary,
#                      by = "wgtcat")
# 
# # Rounding numbers for clarity
# wgtcat_spdf@data[,is.numeric(wgtcat_spdf@data)] <- round(is.numeric(wgtcat_spdf@data), 2)

analysis_final <- analysis
```
## Writing Outputs
```{r writing outputs, message=FALSE, warning=FALSE}
# write.csv(point_weights,
#           file = paste0(output_path, "/",
#                         output_filename,
#                         "_pointweights_",
#                         date, ".csv"),
#           row.names = FALSE)
# write.csv(wgtcat_summary,
#           file = paste0(output_path, "/",
#                         output_filename,
#                         "_wgtcatsummary_",
#                         date, ".csv"),
#           row.names = FALSE)
write.csv(analysis_final,
          file = paste0(output_path, "/",
                        output_filename,
                        "_analysis_",
                        date, ".csv"),
          row.names = FALSE)

arc.write(data = all_points,
          path = paste0(output_path,"/",
                        analysis_gdb,"/",output_filename,
                               "_points"),
          overwrite = TRUE)

arc.write(data = wgtcat_spdf,
          path = paste0(output_path,"/",
                        analysis_gdb,"/",output_filename,
                               "_wgtcats"),
          overwrite = TRUE)
```
## Results
### Figures

#### Plotting weighted analysis results
```{r, fig.width=12, fig.height=8, message=FALSE, eval = FALSE}

# reorder factors
library(scales)

# these 2 are mixeed up
colnames(analysis_final)[c(1,2)] <- c("Indicator", "Rating")

analysis_final$Rating <- as.factor(analysis_final$Rating)

# replace with pretty indicator names
analysis_final$Indicator <-  gsub("wf_baresoil","Bare Soil Cover",analysis_final$Indicator)
analysis_final$Indicator <-  gsub("wf_gap","Large Canopy Gaps (>200cm)",analysis_final$Indicator)
analysis_final$Indicator <-  gsub("wf_soilstab","Soil Stability",analysis_final$Indicator)
analysis_final$Indicator <-  gsub("ep_invasiveag", "Annual Grass Cover",analysis_final$Indicator)
analysis_final$Indicator <-  gsub("hq_veg_ARMPA_PG_Cover","DRPG Cover",analysis_final$Indicator)
analysis_final$Indicator <-  gsub("hq_veg_ARMPA_ARTR2","Sagebrush Height",analysis_final$Indicator)
analysis_final$Indicator <-  gsub("hq_veg_ARMPA_PG","DRPG Height",analysis_final$Indicator)
analysis_final$Indicator <-  gsub("hq_veg_ARMPA","Sagebrush/Shrub Cover",analysis_final$Indicator)

levels(analysis_final$Rating)

g <- ggplot(data = analysis_final,
                aes(y = `Estimated hectares`,
                             x = Rating, col = Rating)) +
  geom_point(position = "dodge", size = 3) +
  geom_errorbar(aes(ymin = `Lower confidence bound of hectares (80%, Goodman multinomial) `,
                                      ymax = `Upper confidence bound of hectares (80%, Goodman multinomial) `),
                         width = 0.2, size = 1.2)+
  facet_wrap(.~Indicator, scales = "free")+
  coord_flip()+
  theme_bw(base_size = 16)  +
  labs(y = "Hectares", x = "")+
  scale_y_continuous(label = comma)+
  theme(panel.spacing.y = unit(2,"lines"))

g

ggsave(plot = g, device = "jpeg", dpi = 400, width = 17, filename = paste0("analysis_hectares_", output_filename, ".jpeg"), path = output_path)

# now with %
g_percent <- ggplot(data = analysis_final,
                aes(y = `Estimated percent of sampled area`,
                             x = Rating, col = Rating)) +
  geom_hline(yintercept =  80, size = 1, linetype = 2)+  # adding the thresold for the proporiton of the landscape meeting
  geom_point(position = "dodge", size = 3) +
  geom_errorbar(aes(ymin = `Lower confidence bound of percent area (80%, Goodman multinomial) `,
                                      ymax = `Upper confidence bound of percent area (80%, Goodman multinomial) `),
                         width = 0.2, size = 1.2)+
  facet_wrap(.~Indicator, scales = "free")+
  coord_flip()+
  theme_bw(base_size = 16)  +
  labs(y = "Hectares", x = "")+
  scale_y_continuous(label = comma)+
  theme(panel.spacing.y = unit(2,"lines"))

g_percent

ggsave(plot = g_percent, device = "jpeg", dpi = 400, width = 17, filename = paste0("analysis_percent_", output_filename, ".jpeg"), path = output_path)

```

